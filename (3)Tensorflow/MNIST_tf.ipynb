{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24616090-b3c0-4c07-8c6a-306a823ecf21",
   "metadata": {},
   "source": [
    "# MNIST_Tf implimentation\n",
    "### This images contains handwritten digits from 0->9\n",
    "### NN has to classify handwritten handWritten digit_images into their respective digits from 0->9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "f8a5b8b7-ab6d-4d39-bbe1-c91ba243a03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193d9fbd-1d08-49d1-8b54-38b945e95bba",
   "metadata": {},
   "source": [
    "## Loading MNIST datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "67439e51-ca01-4f66-9a1e-4d83aba93175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset from tensorflow_keras\n",
    "(X_train,Y_train),(X_test,Y_test) = tf.keras.datasets.mnist.load_data() #Same functionality as below, below shows how its saved\n",
    "## It would give tupple => ([X_train,Y_train],[X_test,Y_test]) , firstIndex have list for training and second has a list for testing \n",
    "mnist = tf.keras.datasets.mnist.load_data()  \n",
    "X_train = mnist[0][0] #Would give X_train \n",
    "Y_train = mnist[0][1] #Would give X_train\n",
    "X_test = mnist[1][0] #Would give X_train\n",
    "Y_test = mnist[1][1] #Would give X_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "37388efd-d4d7-4fce-9338-4dd546464190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Above dataset is like this\n",
    "tupple = ([11,23],[34,45])\n",
    "print(type(tupple))\n",
    "tupple[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "fae8fc1b-5ca4-4995-a907-dee7201da6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n",
      "\n",
      "(60000,)\n",
      "(10000,)\n",
      "[7 2 1 ... 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "# Total 70,000 MNIST handwritten images\n",
    "# (28,28) means 28 width size and 28 height size, total pixels are 784\n",
    "print(X_train.shape) #X are the matrix of size 28x28, having pixel values in them\n",
    "print(X_test.shape)\n",
    "print()\n",
    "# Y refers to the image labels, means which digit is written in the image\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)\n",
    "print(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dfc77a-d413-4827-bcfb-875cd738e770",
   "metadata": {
    "tags": []
   },
   "source": [
    "## OneHot endcoding for Y labels, for multiclass classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "b959c004-5389-439f-93d9-1ec945d6d22d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Y_labels are integers values, it has to be converted into oneHot endcoding, to be get recognised from the neuron\n",
    "#And integer lables have be oneHotEncoded while training\n",
    "Y_train = tf.keras.utils.to_categorical(Y_train , num_classes=10) #OneHot Encoded to 10 classes \n",
    "Y_test = tf.keras.utils.to_categorical(Y_test , num_classes=10) #Same with testingLabels\n",
    "Y_train.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250d4148-2a8b-4f42-92b6-95c6ed12f3e5",
   "metadata": {},
   "source": [
    "## Displaying the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "c2c3fef4-0e3a-4086-a1f1-551782af1c6e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If the images are already flattend means, all pixels_value converted in a 1D-list, then we have to reshped into equal row and column or original row and col, bcz image can only be plotted when they were not flattend\n",
    "# To plot image, row and columns should be known  \n",
    "# But here already they are row and col, so no reshape, if not so, then we had to use .reshape((28,28)), after every image, as parent row,col here is 28,28\n",
    "first_image = X_train[0] #Plotting firstImage\n",
    "print(first_image.shape)\n",
    "plt.imshow(first_image) #Building use matplotlib.pyplot\n",
    "plt.show()\n",
    "Y_train[0] #Label representing 5 (starting from 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "e490b9e1-0c8f-4f4d-80fb-337901158e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "# Reshaping images's size from 2D shape to 1D -> flattening\n",
    "# Current shape of images (no. of images, image.size(28,28))\n",
    "# So passes image parameter, then told that no. of images remain as it is\n",
    "# Then '-1' shows that convert remaing 2D to 1D, compenstae the remaining size by multiplying with other,\n",
    "# 28x28 = 784\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "print(X_train.shape)\n",
    "\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "print(X_test.shape)\n",
    "\n",
    "# # Initialize TensorFlow session\n",
    "# with tf.compat.v1.Session() as sess:\n",
    "#     # Evaluate the tensor within the session to get its value as a numpy array\n",
    "#     X_train_np = sess.run(X_train)\n",
    "#     X_test_np = sess.run(X_test)\n",
    "#     print(X_train_np.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a642198f-95b4-4e18-b6c4-9f4c8e68f251",
   "metadata": {},
   "source": [
    "## Building the NN\n",
    "### All parameters(W,B) would be variables, not placeHolders(As user willnot provide value) and constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "3754c345-2cb4-4944-a9b7-f33c4f89f7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.6604403 0.996176 ]]\n"
     ]
    }
   ],
   "source": [
    "# Creating a array having randomly selected number from normal distribution, \n",
    "# -would use for putting random values in weights \n",
    "tf.compat.v1.disable_eager_execution()\n",
    "sess = tf.compat.v1.Session()\n",
    "print(sess.run(tf.compat.v1.random_normal([1,2]))) #Inside sqBracs required shape, can take shape of any kind\n",
    "# Hence here is no processing of elements with each other is there, so no session req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "b2a38ce7-06e1-4c8d-a2cf-a251e9bae019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intilialising no. of units in each layer\n",
    "n_input = 784\n",
    "n_hidden_1 = 256\n",
    "n_hidden_2 = 256\n",
    "n_output = 10\n",
    "\n",
    "# Initialsing weightsDictionary, in which key is layer, and value will be array of the weights of that layer \n",
    "weights = {\n",
    "    'h1':tf.Variable(tf.compat.v1.random_normal([n_input,n_hidden_1])), #Hence got 2D list with 784_rows(input_neuron) and 256_cols(H1_Neuron) \n",
    "    'h2':tf.Variable(tf.compat.v1.random_normal([n_hidden_1,n_hidden_2])), #265x256\n",
    "    'out':tf.Variable(tf.compat.v1.random_normal([n_hidden_2,n_output])) #256x10\n",
    "}\n",
    "\n",
    "# Initialsing BiasesDictionary, in which key is layer, and value will be array of the biases for that layer \n",
    "biases = {\n",
    "    'h1':tf.Variable(tf.compat.v1.random_normal([n_hidden_1])), #1D list having 1x256 parameter of biases\n",
    "    'h2':tf.Variable(tf.compat.v1.random_normal([n_hidden_2])), # 1x256\n",
    "    'out':tf.Variable(tf.compat.v1.random_normal([n_output])) # 1x10\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07b5c19-94c1-46a7-a915-d3f423461e31",
   "metadata": {},
   "source": [
    "## Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "faebec66-d6cc-494f-b734-8d2d194ed4ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function tensorflow.python.ops.gen_nn_ops.relu(features, name=None)>"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.relu # Relu-Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "958ca4a2-b9f7-4403-ba45-79e89aa18e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardPropagation(X , weight , biases):\n",
    "    Input_h1 = tf.matmul(X , weights['h1']) + biases['h1']\n",
    "    Output_h1 = tf.nn.relu(Input_h1)\n",
    "    \n",
    "    Input_h2 = tf.matmul(Output_h1 , weights['h2']) + biases['h2']\n",
    "    Output_h2 = tf.nn.relu(Input_h2)\n",
    "    \n",
    "    Input_out = tf.matmul(Output_h2, weights['out']) + biases['out']\n",
    "    return Input_out # Output has no actication function\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "e2da225e-07bc-4a4c-95e7-11f42ec8ea6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f420001-1f62-4c6d-bdbb-e9d70966115a",
   "metadata": {},
   "source": [
    "### Running Fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "ff8935d5-0aee-4c91-9c94-99e346df40bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder_142:0\", shape=(None, 784), dtype=float32) Tensor(\"Placeholder_143:0\", shape=(None, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Applying placeholders in X and Y, so that we could change from training to testing when we want\n",
    "# Both train and test, no. images(rows) are different but columns(Flattend_size) is same\n",
    "sess = tf.compat.v1.Session()\n",
    "# sess.run(tf.compat.v1.global_variables_initializer()) #In case of variable intilisation , have to call this command, which makes above variables as global\n",
    "X = tf.compat.v1.placeholder(tf.float32,[None,784]) # None means, any number could come\n",
    "Y = tf.compat.v1.placeholder(tf.float32,[None, 10])\n",
    "\n",
    "pred = forwardPropagation(X , weights, biases)\n",
    "sess.run(tf.compat.v1.global_variables_initializer()) #In case of variable intilisation , have to call this command, which makes above variables as global\n",
    "\n",
    "#Hence output would be a Matrix same as Y_true of size 10, so we have to find that index having maxValue at each row, means vertically, \n",
    "# - so axis would be 1\n",
    "prediction = tf.argmax(pred, axis=1)\n",
    "true_labels = tf.argmax(Y, axis=1)\n",
    "\n",
    "#Accuracy\n",
    "correct_predictions = tf.equal(prediction , true_labels) #Will return boolean list, if same true es=lse false\n",
    "print(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae8869c-057d-48da-802f-822b7f583690",
   "metadata": {},
   "source": [
    "### Prediction and performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "0e7ae03e-62e6-4709-81ca-7483d83d5b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,)\n",
      "[2 2 2 ... 2 2 2]\n",
      "(60000,)\n",
      "[5 0 4 ... 5 6 8]\n",
      "[False False False ... False False False]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The correct matches are :6574'"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now running the session to execute the implimentation/process\n",
    "sess = tf.compat.v1.Session()\n",
    "sess.run(tf.compat.v1.global_variables_initializer()) #In case of variable intilisation , have to call this command, which makes above variables as global\n",
    "\n",
    "prediction_evaluated , labels , correctness= sess.run([prediction,true_labels,correct_predictions], feed_dict={X: X_train, Y: Y_train})\n",
    "print(prediction_evaluated.shape)\n",
    "print(prediction_evaluated)\n",
    "print(labels.shape)\n",
    "print(labels)\n",
    "print(correctness)\n",
    "\n",
    "# Check how many correct \n",
    "f\"The correct matches are :{correctness.sum()}\" #Would sum the items in list, as True represent = 1 and false = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4959db0d-7972-4d3e-aec1-c000796c4efd",
   "metadata": {},
   "source": [
    "## Backward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae33f577-6186-4835-9d09-721547573344",
   "metadata": {},
   "source": [
    "### Defining the cross_entropy loss cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "2be8f5b3-093e-45b8-a5d4-57c876f092f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "437909.97\n",
      "[629709.44 748496.1  745062.4  ... 593984.56  21177.   519320.62]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Found the mean off the costFunction as its was containg the 10K or 60K losses\n",
    "# TO reduce this to one, founded the mean of all \n",
    "cost_withOut_Mean = tf.nn.softmax_cross_entropy_with_logits(logits=pred , labels = Y) # Would give list/tuple of 10k or 60k losses, losses of each datapoints \n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred , labels = Y)) # Would find the mean of all loses \n",
    "cost_value , not_meaned_cost = sess.run([cost,cost_withOut_Mean], feed_dict={X: X_train, Y: Y_train})\n",
    "print(cost_value)\n",
    "print(not_meaned_cost)\n",
    "type(not_meaned_cost.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f49cc9d-50d7-4e00-be03-8c7257bbdbc8",
   "metadata": {},
   "source": [
    "### Intilalising the AdamOptimizer, which gonna minimize the cost from gradientDescnt tehcnique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "1051f164-e428-40ff-9fa3-d7402d3c0021",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=0.01)\n",
    "optimize = optimizer.minimize(cost)\n",
    "sess.run(tf.compat.v1.global_variables_initializer()) #In case of variable intilisation , have to call this command, which makes above variables as global\n",
    "# Above required for Optmizer, as its a variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecfb37c-dc24-48e8-809d-f436938071da",
   "metadata": {},
   "source": [
    "### Now running the optimizer, for whole datapoints then after changing the weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "fa1e621a-b34c-479e-bae5-6d23bd40f8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Itertavily change call the optimzer which would find the global minima and minimize the cost\n",
    "for i in range(100):\n",
    "    c,_=sess.run([cost,optimize], feed_dict={X:X_train , Y:Y_train}) #Processing the optimizer with cost\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "f5d3f028-7203-4719-9b56-b9fe9e96de18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,)\n",
      "[5 0 4 ... 5 6 8]\n",
      "(60000,)\n",
      "[5 0 4 ... 5 6 8]\n",
      "[ True  True  True ...  True  True  True]\n",
      "Correct : 60000\n",
      "Missed : 0\n"
     ]
    }
   ],
   "source": [
    "prediction_evaluated , labels , correctness= sess.run([prediction,true_labels,correct_predictions], feed_dict={X: X_train, Y: Y_train})\n",
    "print(prediction_evaluated.shape)\n",
    "print(prediction_evaluated)\n",
    "print(labels.shape)\n",
    "print(labels)\n",
    "print(correctness)\n",
    "print(\"Correct :\",np.sum(correctness))\n",
    "print(\"Missed :\",X_train.shape[0]-np.sum(correctness))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05146526-bbc3-4a73-8bc3-9a24e0c28ea8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Now running the optimizer for small batches first then changing the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4fb045-1553-4c87-bb11-6d8326e08df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Itertavily change call the optimzer which would find the global minima and minimize the cost \n",
    "# And change the parameters after every batch of datapoints\n",
    "batch_size = 100\n",
    "# Total bacthes would be 60,000/100=600, so the optimizer would run over the batch\n",
    "for i in range(25):\n",
    "    num_batches = X_train.shape[0]//batch_size\n",
    "    c,_=sess.run([cost,optimize], feed_dict={X:X_train , Y:Y_train}) #Processing the optimizer with cost\n",
    "print(c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
