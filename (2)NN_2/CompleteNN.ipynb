{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bf4c9c4-c490-40d6-904d-6dbcbaa4d2be",
   "metadata": {},
   "source": [
    "# Implimenting Forward propagation and back propagation for NN with hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2733e243-e8d1-414d-abc1-87829f2fc758",
   "metadata": {},
   "source": [
    "## Implimenting XOR function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec267b59-0910-4b13-9f99-dca1ef58c783",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "993c629c-de1c-4384-ba53-3e6d533bd67e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "Y = np.array([[0,1,1,0]]).T\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea8710c2-5062-410a-967d-f031962ec825",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sig(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5789737-cdeb-4a27-a726-78ed044955cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivativeSig(z):\n",
    "    return sig(z)*(1-sig(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "285548da-c07c-4d87-91e8-f7b0810bd649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hidden layer : Weights from 2 input neuron to 2 hidden neuron\n",
    "wH = 2*np.random.random((2,2))-1\n",
    "bH = 2*np.random.random((1,2))-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d39bead-1673-4194-94b6-a882db180f24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output Layer weights : from 2 hidden neurons to 1 output neuron\n",
    "wO = 2*np.random.random((2,1))-1\n",
    "bO = 2*np.random.random((1))-1\n",
    "lr = 0.1\n",
    "wO.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "bfd88990-af84-4aad-9a3d-6fbb6e833a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02391121]\n",
      " [0.97908542]\n",
      " [0.97909868]\n",
      " [0.02498912]]\n",
      "[[ 7.05047533 -5.37056001]\n",
      " [ 7.03956789 -5.36825797]] [[-3.16636353  8.04352205]]\n",
      "[[8.64629365]\n",
      " [8.81134531]] [[-12.86747661]]\n"
     ]
    }
   ],
   "source": [
    "# Implimenting forward propagation with oneHidden layer\n",
    "for iter in range(10000):\n",
    "    output_InputLayer = X\n",
    "    input_HiddenLayer = np.dot(output_InputLayer,wH)+bH\n",
    "    output_HiddenLayer = sig(input_HiddenLayer)\n",
    "    input_OutputLayer = np.dot(output_HiddenLayer,wO)+bO \n",
    "    output = sig(input_OutputLayer)\n",
    "    # print(output.shape)\n",
    "\n",
    "\n",
    "    # Implimentation of backPropagtion of this NN\n",
    "    ## FirstTwo terms for outputLayer\n",
    "    firstTerm_OutputLayer = output - Y\n",
    "    secondTerm_OutputLayer = derivativeSig(input_OutputLayer)\n",
    "    firstTwo_OutputLayer = firstTerm_OutputLayer * secondTerm_OutputLayer\n",
    "\n",
    "    ## So now we would backpropagate the firstTwo terms from outputLayer to hiddenLayer\n",
    "    firstTerm_HiddenLayer = np.dot(firstTwo_OutputLayer, wO.T)\n",
    "    secondTerm_HiddenLayer = derivativeSig(input_HiddenLayer)\n",
    "    firstTwo_HiddenLayer = firstTerm_HiddenLayer*secondTerm_HiddenLayer\n",
    "\n",
    "    ## To find the changesWeights netween outputLayer and hidden layer, the dot prdt of firstTwo of outputLayer with output of hidden layer \n",
    "    changes_Output_Hidden = np.dot(output_HiddenLayer.T, firstTwo_OutputLayer)\n",
    "    changes_Output_Hidden_Bias = np.sum(firstTwo_OutputLayer,axis=0,keepdims = True)\n",
    "    ### The {keepdims} would do that after the summation we should not get the dimnesionless value(scaler) it would let the \"sum\" same dimension as of its parent so that vecot addition works fine \n",
    "    ### The axis = 0, summation would done by adding each row in the col , in 1 changes by column\n",
    "\n",
    "    ## Same above concept for the weightsChanges between hidden and input\n",
    "    changes_Hidden_input = np.dot(output_InputLayer.T, firstTwo_HiddenLayer)\n",
    "    changes_Hidden_input_Bias = np.sum(firstTwo_HiddenLayer,axis=0,keepdims = True) ### Same concept added as above \n",
    "\n",
    "    ## Now updation of the paramerts\n",
    "    wO = wO - lr * changes_Output_Hidden\n",
    "    bO = bO - lr * changes_Output_Hidden_Bias\n",
    "\n",
    "    wH = wH - lr * changes_Hidden_input\n",
    "    bH = bH - lr * changes_Hidden_input_Bias\n",
    "\n",
    "output_InputLayer = X\n",
    "input_HiddenLayer = np.dot(output_InputLayer,wH)+bH\n",
    "output_HiddenLayer = sig(input_HiddenLayer)\n",
    "input_OutputLayer = np.dot(output_HiddenLayer,wO)+bO \n",
    "output = sig(input_OutputLayer)\n",
    "print(output)\n",
    "print(wH,bH)\n",
    "print(wO,bO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "30b1f81e-cea9-49d1-8549-59c987939a57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 2)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## So now we would backpropagate the firstTwo terms from outputLayer to hiddenLayer\n",
    "# firstTerm_OutputLayer = output - Y\n",
    "# secondTerm_OutputLayer = derivativeSig(input_OutputLayer)\n",
    "# firstTwo_OutputLayer = firstTerm_OutputLayer * secondTerm_OutputLayer\n",
    "\n",
    "# output_InputLayer = X\n",
    "# input_HiddenLayer = np.dot(output_InputLayer,wH)+bH\n",
    "# output_HiddenLayer = sig(input_HiddenLayer)\n",
    "# input_OutputLayer = np.dot(output_HiddenLayer,wO)+bO \n",
    "# output = sig(input_OutputLayer)\n",
    "    \n",
    "firstTwo_HiddenLayer.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
