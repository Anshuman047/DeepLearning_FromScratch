{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "946920e2-10e5-4265-a096-47af4fd23322",
   "metadata": {},
   "source": [
    "# Implimnetation of Backpropagation in NN having no hidden layer, just input layer and hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9052f9-4bc5-44b5-b282-6f01303a637f",
   "metadata": {},
   "source": [
    "## (*)First writing forwar prop code, as at starting random weights are assumed and forward propagate and when get y_pred, then calculate error and backpropagate , from gradient descent to get optimum value of weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "d60faf7b-2202-4df4-b4d2-85f0242f6844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraies\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "65f6dcc4-2a9c-4ee3-8b07-1edeb76c7607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2)\n",
      "(4, 1)\n"
     ]
    }
   ],
   "source": [
    "# Arraning our inputs and output\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "Y = np.array([[0,0,0,1]])\n",
    "Y = Y.T\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "eed19f81-80f0-465f-8d76-ece445a2a46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining sigmoid function\n",
    "def sig(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "bba2f921-c690-4b7c-90c6-c0cfed7e606a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining derivative of sigmoid\n",
    "def derivativeSig(z):\n",
    "    return sig(z)*(1-sig(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89c2b87-5c19-4b4c-917d-9c46026a58b2",
   "metadata": {},
   "source": [
    "## Defining weights and biases for no hidden layer NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "8dc5c07d-d840-4c93-8d5d-c779efdba36c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.56567058],\n",
       "        [0.45273601]]),\n",
       " array([-0.95462463]))"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomly declaring them in range from -1 to 1\n",
    "weights = 2*np.random.random((2,1)) - 1 \n",
    "biase = 2*np.random.random(1)-1\n",
    "weights , biase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "569a4ba4-669b-4b74-a640-8f2f22ea74b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.27795572]\n",
      " [0.37709694]\n",
      " [0.40396912]\n",
      " [0.51594009]]\n"
     ]
    }
   ],
   "source": [
    "# Defining the connection between input to output and finding prediction\n",
    "outputInputLayer = X\n",
    "output = sig(np.dot(outputInputLayer,weights) + biase)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee32595-e7fe-44ba-aae5-e0f715b35eeb",
   "metadata": {},
   "source": [
    "## Above was for the forwardPropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f53bab-ca48-4585-8a39-9887d380ed42",
   "metadata": {},
   "source": [
    "## Now towards backward propagation,\n",
    "### *) We would do forward prop once then change the derivative we have, then again same , similarly like we do it in gradient descent multiple time in the form of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "bbc5c4be-7cbb-4198-a64c-dd4b7c433ddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.27795572],\n",
       "       [ 0.37709694],\n",
       "       [ 0.40396912],\n",
       "       [-0.48405991]])"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating first term og the derivative\n",
    "firstTerm = output - Y\n",
    "firstTerm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "25ef9d22-b3fe-4071-9084-f218bb8c48a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.24523279],\n",
       "       [0.24131884],\n",
       "       [0.24007169],\n",
       "       [0.23407408]])"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating the secondTerm of the derivative\n",
    "Oj = output\n",
    "secondTerm = derivativeSig(Oj)\n",
    "secondTerm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "e8941966-e6da-4a5b-b87f-ed9e6fd4aa62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.06816386],\n",
       "       [ 0.0910006 ],\n",
       "       [ 0.09698155],\n",
       "       [-0.11330588]])"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now lets calculate the product of first-two term\n",
    "firstTwo = firstTerm * secondTerm\n",
    "print(firstTwo.shape)\n",
    "firstTwo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "53465c65-c240-49f2-9c8e-d1817368066e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.00000000001 # Defined learningRate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d1facb-bea4-4aa4-bb87-0e303bbc7f3e",
   "metadata": {},
   "source": [
    "## Lets multiply the firstTwo with third which is input matrix, for respective columns represent respective weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "5d3d07fd-0888-4ad0-ae3d-03c558c5d242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.01632433]\n",
      " [-0.02230528]]\n",
      "\n",
      "[[0.56567058]\n",
      " [0.45273601]]\n"
     ]
    }
   ],
   "source": [
    "# Lets get the altered separte matrix for each weight\n",
    "changes = np.array([[0.0],[0.0]]) # Variable saving weightMatrix same dimnesion (2,1) and then saved to real weightMatrix variable above   \n",
    "\n",
    "for i in range (2):\n",
    "    for j in range(4):\n",
    "        changes[i][0] += firstTwo[j][0] * X[j][i]\n",
    "print(changes)# Containing new altered weights retrived from the derivative of error function wrt weights \n",
    "print()\n",
    "# Updating the weights from the new ones\n",
    "weights = weights - lr * changes\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6732c7-61fb-4ad3-916b-6a699e93a0e5",
   "metadata": {},
   "source": [
    "## Lets do same for the new altered biase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "03554a61-2c8c-49bb-91cc-324f5f8ef4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.14284012]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.95462463])"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Everything would be same as its also caontaining weights, one thing different that is value of the Inputneuron is 1 \n",
    "changes_biase = np.array([0.0])\n",
    "\n",
    "for j in range(4):\n",
    "    changes_biase[0] += firstTwo[j][0] * 1\n",
    "    \n",
    "print(changes_biase)\n",
    "biase =biase - lr * changes_biase\n",
    "biase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d858d05-ff29-4741-9b00-eb7fdd8530a1",
   "metadata": {},
   "source": [
    "## Now lets run forward and back propagation for many times to reach to global minima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "a22dbd00-26be-4f71-8182-7ef94ce86fa8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.27795573]\n",
      " [0.37709695]\n",
      " [0.40396913]\n",
      " [0.5159401 ]]\n",
      "[[0.56567058]\n",
      " [0.45273602]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.95462458])"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for iter in range(10000):\n",
    "    output = sig(np.dot(X,weights)+biase)\n",
    "    first = output - Y\n",
    "    second = derivativeSig(output)\n",
    "    firstTwo = first * second\n",
    "    \n",
    "    weightsNew = np.array([[0.0],[0.0]])\n",
    "    biaseNew = 0.0\n",
    "    \n",
    "    for i in range(2):\n",
    "        for j in range(4):\n",
    "            weightsNew += firstTwo[j][0] * X[j][i] \n",
    "    weights = weights - lr*weightsNew\n",
    "    \n",
    "    for i in range(4):\n",
    "        biaseNew += firstTwo[j][0] * 1\n",
    "    biase = biase - lr*biaseNew\n",
    "    \n",
    "output = sig(np.dot(X,weights)+biase)\n",
    "print(output)\n",
    "print(weights)\n",
    "biase"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
